
\section{Dynamcial systems framework}
\label{dynamics}

\lhead{Chapter 1. \emph{Dynamical systems}} 

%----------------------------------------------------------------------------------------
%	subsection 1
%----------------------------------------------------------------------------------------

Dynamical systems theory deals with time varying systems. It dwells on questions of how the behaviour of systems evolve over time. Non-linear dynamics is a powerful analytic tool for studying complex systems. A network of neurons can be formalized as a system of differential equations, lending them amenable to be studied in the same framework. A network with recurrent connections is a dynamical system. Some models of hippocampal phenomena derive insights form dynamical systems approach. \\
There is the long standing idea that interesting abstract properties emerge in a population of interacting objects obeying certain local rules. This framework might prove to be useful in developing further insights into the mechanisms that give rise to emergent cognitive faculties.\\
\\
Differential equations provide a convenient and straight forward technique to formalize and study dynamical systems. The system under study is formulated by specifying governing equations capturing the dynamics of the system. The variables in these equations are called state variables. Solving these equations given initial conditions, we can predict the future states of the system. One soon realizes that this is no trivial matter for complex systems. Some systems are inherently unpredictable event though the governing equations are deterministic. Higher order differential equations can be converted to a system if first order differential equations. The system of differential equations can be imagined as a vector field in the state space where a vector is assigned to every point in the state space. The vector points in the direction of change of the state variables and its magnitude conveys the rate of change of the state. 
 
\subsection{Basic concepts}

\subsubsection{Fixed points}
When some systems attain certain states, they show no further change provided that there is no external perturbation. These states are called the $fixed points$ of the system i.e where the flow is zero and consequently the state remains fixed.  \st{this figure shows a one dimensional system whose vector field is defined by some arbitrary function $f(x)$ When $f(x)$ is positive the system moves towards $+ \inf$ when $f(x) = 0$, there is no change. These states are called the fixed points of the system.} A fixed point is stable if the system can recover form small perturbations about the fixed point.  An unstable fixed point is one about which there is no restoring force that steers the system back to the fixed point.  A mechanical equivalent of unstable fixed point is an inverted pendulum.

%\st{When there are two or more state variables, different types of fixed points are observed : \\
%\begin{itemize}
%\item star node
%\end{itemize}
%fig- show a two dimensional system where different types of fixed points result when the parameter %$a$ is varied. These are the stable nodes star node , line of fixed points , saddle node.} \\


Usually the behavior of a dynamical system can be predicted from its energy functional, if it exists. It is a scalar function of the state variables. Without external input, any system always tends to evolve to a state with lower energy. The standard picture is to imagine a ball rolling down the hill towards the valley and always ending up in the valley. The energy landscapes can be complex in biological systems. Some systems are highly sensitive to initial conditions. The ball can end up in one of the several valleys depending on where it starts. A simple example is that of bi-stable system which has been used for modelling working memory. One of the states is the resting state, other is with persistent activity which could function as working memory.\\
The stable fixed points of the systems are interesting since if they exist, the transients die out and the system will eventually settle to one of the stable states. These stable states are called attractors of the system. An attractor is the limiting set of states that the system will approach as $t\, \rightarrow \, \infty $. The topology of the attractors can be helpful in predicting the long term behaviour of a system. A point attractor is a single point in the state space towards which all system evolution trajectories converge. A line attractor is a single continuous arrangement of point attractors. Limit cycle attractors give rise to periodic behavior of the systems. Limit cycles cannot occur in linear systems. Biological pattern generators can be understood in terms of limit cycles. Rhythmic spiking neurons have stable limit cycle attractors.  \st{Here is an example of modified HH model... x axis is the membrane voltage , y is teh potassium activation variable, Initially th model neuron is at rest corresponding to a stable point. If a strong pulse of current is injected in the membrane , it will take the neuron to the basin of attraction of the limit cycle and the neuron will produce rhythmic spikes.} \\
\st{point. line attractor If end points of the line attractor are joined, a ring attractor is obtained which is proposed a model of the head direction system.} \\

\subsubsection{Stability analysis}
The stability of fixed points can be analyzed by linearizing the system at the fixed point and then applying linear stability analysis techniques. This gives correct predictions of stability around the fixed point. When linear methods fails, Layapnov stability analysis approach is usually adopted. Lyapnov function provides a generalized energy landscape and conservative estimate of domains of attraction . 
\\
\subsubsection{Bifurcations}
Certain parameters of systems when varied, result in it exhibiting qualitatively new behavior, the system is then said to have undergone a bifurcation. Bifurcations reflect the dependence on parameters. At some critical values of the parameters, the system switches between different regimes of operation. So the phase space can be partitioned into regions with qualitatively distinct behaviors.\\In neural networks, the same network could be switched between different regimes to implement different computations, where neuro-modulators levels might be responsible for switching.\\ Bifurcations are classified as local and global. Global if the bifurcation effects a large portion of the state space. Bifurcations are classified based on how the properties of fixed points change.  
\begin{itemize}
\item \emph{Saddle node bifurcation} occurs when a stable and an unstable fixed points get closer and closer as the parameter is varied , eventually colliding and vanishing. \st{ variable and the fixed points are plotted as the dependent variable. The dotted lines indicate the unstable nodes and the stable mode.}

\item \emph{pitchfork bifurcation} occurs in symmetric systems as the parameter is varied it loses stability and two new fixed points are created. \st{Eg - a beam buckling when overloaded load}

\item \emph{Hopf bifurcation} occurs when a stable spiral loses its stability and a limit cycle is created. \st{model neuron the injected current as the parameter , when ramp current is applied at a critical value the spiking.}

\end{itemize}
The hippocampus exhibits different states when the animal is mobile and exploring its environment. When the animal is immobile or sleeping, the activity is qualitatively different \cite{Buzsaki2011,  Montgomery2008}.
At the network level, this type of transition between different network states could be brought about by the network undergoing bifurcations. 

\subsection{Attractor networks}
Attractor networks have been theoretically shown to have several computational capabilities \cite{Amit1992}, especially for computations in noisy systems.\\
Auto-assosiative memory and map based path integration, functions that are attributed to the hippocampus; have been hypothesized to be implemented in the attractor network of the CA3. Attractor networks have stable patterns as their attractor sets and depending on the initial conditions the network will settle down to one of the stable patterns. Depending on the type of attractor different kinds patterns can be achieved. Point attractors serve to memorize patterns. Localized activity patterns are achieved by local excitation and long range inhibition. If the point attractors are very close to each other, this will result in a continuous attractor \cite{Trappenberg2003}. It is possible to stabilize activity at every point along a continuous attractor. Continuous attractor dynamics has been proposed as the underlying dynamics for path integration (sec. \ref{pathIntgr}, p. \pageref{pathIntgr}). Several other experimentally observed hippocampal phenomena could be explained as a signature of attractor dynamics, where simple feed-forward network explanations fail.\\

\subsubsection{Auto-associative network}
\label{autoasso}
For reliable recall of memories, pattern completion capabilities would be advantageous for a memory system. The recurrent synaptic connectivity of CA3 is suitable for implementing an auto associative network. An auto-associative network memorizes patterns through local learning rules that modify the synaptic strengths. Each pattern produces an energy minima in the energy landscape. Pattern completion occurs because of the recurrent synaptic architecture. When a few neurons of a certain pattern are active, then they activate others because of strong synaptic connectivity strength. CA3 representation is found to be coherent between environments with small changes in cues \cite{Lee2004, Vazdarjanova2004}, which could be explained as an effect of pattern completion. \\
Experiments which observe the phenomenon of \emph{remapping} \cite{Kubie1987, Wills2005, Leutgeb2005} (sec. \ref{remapping}, p. \pageref{remapping})have put to doubt the auto-associative function usually ascribed to  hippocampus \cite{Colgin2010}. Global remapping might be caused when the path integration system coordinates are reset, which may be brought about by changing of location.   

\subsubsection{Models of place selectivity and phase precession}
Some models of place cells are constructed as networks with attractor dynamics. The basic requirement to get place cell like activity is to obtain localized firing patterns, so that the this pattern might then be utilized to encode distinct locations in the physical space. Second requirement is to have a mechanism by which this localized activity can be conveniently updated as the animal traverses in the environment.  \\

Contrary to the view that there is an explicit temporal code (sec. \ref{placeCells} p. \pageref{placeCells}), it has been suggested that the correlation between position and phase of theta could be explained as a consequence of sequential computation occurring within a theta cycle. Across the population of place cells, different phases the theta cycle encode positions offset into either future or past along the rat's trajectory \cite{Itskov2008}. The past and future locations robustly predict the CA1 place cell activity at different theta phases. This indicates that the information content in the cell activity at different phases of theta is actually correlated with the past and future locations of the animal. It was also show that this phenomena is not a direct consequence of phase precession, rather might actually be a causing the observed phase precession. \\

Tsodyks \cite{Tsodyks1996} proposed a model proposing possible mechanisms through with place cell selectivity is achieved through a specific synaptic connectivity between neurons reflecting the distances between their place field peaks in the environment. In the presence of global inhibition this architecture results in attractor dynamics. External input with weak selectivity is sufficient to steer the network into the one of the basins of attraction resulting in stable localized activity. The activity tracks the input peak location as it moves. With the addition of asymmetric synaptic strengths in the direction of motion, phase precession effect is observed. Hence the phase precession could be a manifestation of the inherent asymmetry in the synaptic connections. The inhibitory interneurons in the model network receive theta input which is the simulated input from medial septum to the GABAergic interneurons. The simulation of the network shows that as the rat moves along the track the external excitation drifts through a group of neurons. Due to the asymmetry in the synaptic connections the activity spontaneously propagates forward in every theta cycle. \\  

\subsubsection{Internal sequences in the rat hippocampus}
In the study by Pastalkova \cite{Pastalkova2008a}, multi-unit recordings were obtained form the rat hippocampus in non-sleep state. They report observation of internally self sustained cell assembly sequences. This has been modeled as a network with attractor dynamics. The animals were trained to run in a wheel during the delay period in an alternation task. The CA1 pyramidal neurons were recorded during the delay period. Some of these neuron assemblies were sequentially activated. These sequences were predictive of the time the rat spent in the wheel upto 20 seconds \cite{Itskov2011a}. The sequences were unique for different behavioral choices including the ones which were incorrect choices. Since the location of the rat was stationary, one would expect to see only place cells for that location to be active.  \\  
%[time/distance cells]
It has been proposed that this could be a means by which the networks keeps track of time elapsed. A model was proposed suggesting a possible mechanism for the generation of cell sequences in a network with no strong inputs \cite{Itskov2011a}. The two critical ingredients of the model are adaptive thresholds and Mexican hat synaptic connectivity. In the model, every spike fired by the neuron will result in the increase of its spiking threshold. This threshold then exponentially decays to its default value with a time constant in the order of seconds. Thus a neuron gets increasingly discouraged to fire as its firing rate increases, which will eventually silence the neuron.The Mexican hat type connectivity ensures that the activity remains localized to a small number of neighboring neurons. The symmetry in the synaptic weights is broken by introducing uncorrelated noise in the connectivity matrix. In fact, to ensure that the dynamics displayed by the model is not just a result of the perfect synaptic tuning; the strength of the correlated Mexican hat connectivity was chosen to be weaker as compared to that of the heterogeneous component. This model generates a continuum of bump attractors, for the network, this implies that if the bump were to be moved laterally by some means, it would stabilize in the new position.  The introduction of threshold adaptation will result in the increase of the thresholds of neurons participating in the localized activity. Gradually, on the time scale of seconds (i.e. the time scale of threshold relaxation), this bump will lose its stability as the neurons become quite due high values of threshold. Assuming the input noise levels are low, the heterogeneity in the network connectivity will dictate the next position of the stable bump, which will remain so until it is destabilized again by the same mechanism. The bump in essence is moving away from the neurons with recently updated thresholds. In this setting, activity bump shifts its peak constantly without ever stabilizing at a particular location. Thus the model exhibits self generated sequential activation of cells, captured by the bump constantly moving along continuous trajectories in the state space. The model produces reliable trajectories even with weakly noisy input, provided that it starts with the same initial conditions. Hence, the similar heterogeneity and threshold levels across trials provide identical initial conditions (contexts) which results in reproducible behavior. It was also shown that these sequences can be inherited by succeeding layer of neurons without recurrent connections receiving sparse feed-forward input from the layer with recurrent connectivity. Since the CA1 region does not have recurrent connectivity and the sequences were observed in CA1, it is possible that they are inherited form sequences are generated elsewhere.


\subsubsection{Preplay}
The trajectories taken by an animal are rapidly encoded as sequences of place cells. It has been proposed that this can be accomplished if there exists a pool of sequences ready to be bound to sensory cues. The mechanism of \emph{preplay} has been reported as evidence for this claim  \cite{Dragoi2011, Dragoi2013a}. 
